#!/usr/bin/env python3
"""
üöÄ COMPLETE DEPLOYMENT AUDIT SUITE
==================================

This script runs all available tests on the DigitalOcean deployment and provides
a comprehensive audit with actionable recommendations.

Usage: python run_complete_deployment_audit.py [URL]
"""

import sys
import subprocess
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DeploymentAuditor:
    """Complete deployment audit orchestrator"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip('/')
        self.test_results = {}
        self.audit_timestamp = datetime.now()
        
    def run_test_script(self, script_name: str, test_name: str) -> Dict:
        """Run a test script and collect results"""
        logger.info(f"üîç Running {test_name}...")
        
        try:
            # Check if script exists
            script_path = Path(script_name)
            if not script_path.exists():
                logger.warning(f"‚ö†Ô∏è Test script {script_name} not found, skipping...")
                return {"error": f"Script {script_name} not found", "skipped": True}
            
            # Run the test script
            result = subprocess.run(
                [sys.executable, script_name, self.base_url],
                capture_output=True,
                text=True,
                timeout=120  # 2 minute timeout
            )
            
            # Parse output
            exit_code = result.returncode
            stdout = result.stdout
            stderr = result.stderr
            
            # Look for JSON report files generated by the test
            report_files = []
            for pattern in [
                f"*test_report_{self.audit_timestamp.strftime('%Y%m%d')}*.json",
                "*test_report_*.json"
            ]:
                report_files.extend(Path(".").glob(pattern))
            
            # Get the most recent report
            latest_report = None
            if report_files:
                latest_report_file = max(report_files, key=lambda p: p.stat().st_mtime)
                try:
                    with open(latest_report_file, 'r') as f:
                        latest_report = json.load(f)
                except Exception as e:
                    logger.warning(f"Could not read report file {latest_report_file}: {e}")
            
            return {
                "exit_code": exit_code,
                "success": exit_code == 0,
                "stdout": stdout,
                "stderr": stderr,
                "report": latest_report
            }
            
        except subprocess.TimeoutExpired:
            logger.error(f"‚ùå {test_name} timed out after 2 minutes")
            return {"error": "Test timed out", "success": False}
        except Exception as e:
            logger.error(f"‚ùå {test_name} failed to run: {e}")
            return {"error": str(e), "success": False}
    
    def run_all_tests(self) -> Dict:
        """Run all available test suites"""
        logger.info("üöÄ Starting Complete Deployment Audit")
        logger.info(f"üìç Target URL: {self.base_url}")
        logger.info(f"‚è∞ Started: {self.audit_timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 80)
        
        # Define all test suites
        test_suites = [
            ("test_digitalocean_deployment.py", "DigitalOcean Deployment Test"),
            ("test_frontend_automation.py", "Frontend Automation Test"),
            ("comprehensive_zerodha_audit.py", "Zerodha Authentication Audit")
        ]
        
        # Run each test suite
        for script_name, test_name in test_suites:
            self.test_results[test_name] = self.run_test_script(script_name, test_name)
        
        # Generate comprehensive summary
        return self.generate_comprehensive_report()
    
    def generate_comprehensive_report(self) -> Dict:
        """Generate a comprehensive audit report"""
        logger.info("\n" + "=" * 80)
        logger.info("üìä GENERATING COMPREHENSIVE AUDIT REPORT")
        logger.info("=" * 80)
        
        # Analyze results
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results.values() if result.get("success", False))
        
        # Calculate overall health score
        overall_score = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # Determine overall status
        if overall_score >= 90:
            status = "EXCELLENT üü¢"
            status_emoji = "üü¢"
        elif overall_score >= 75:
            status = "GOOD üü°"
            status_emoji = "üü°"
        elif overall_score >= 50:
            status = "FAIR üü†"
            status_emoji = "üü†"
        else:
            status = "POOR üî¥"
            status_emoji = "üî¥"
        
        # Extract key metrics from individual test reports
        metrics = self.extract_key_metrics()
        
        # Generate recommendations
        recommendations = self.generate_recommendations()
        
        # Identify critical issues
        critical_issues = self.identify_critical_issues()
        
        comprehensive_report = {
            "audit_metadata": {
                "timestamp": self.audit_timestamp.isoformat(),
                "target_url": self.base_url,
                "audit_version": "1.0",
                "total_test_suites": total_tests
            },
            "overall_assessment": {
                "status": status,
                "score": f"{overall_score:.1f}%",
                "successful_tests": successful_tests,
                "total_tests": total_tests
            },
            "test_suite_results": self.test_results,
            "key_metrics": metrics,
            "critical_issues": critical_issues,
            "recommendations": recommendations,
            "next_steps": self.generate_next_steps(overall_score)
        }
        
        # Print executive summary
        self.print_executive_summary(comprehensive_report)
        
        return comprehensive_report
    
    def extract_key_metrics(self) -> Dict:
        """Extract key metrics from all test reports"""
        metrics = {
            "deployment_health": "Unknown",
            "frontend_functionality": "Unknown",
            "zerodha_auth_status": "Unknown",
            "api_endpoints_working": "Unknown",
            "database_connected": "Unknown",
            "redis_connected": "Unknown"
        }
        
        # Extract from DigitalOcean deployment test
        deployment_test = self.test_results.get("DigitalOcean Deployment Test", {})
        if deployment_test.get("report"):
            report = deployment_test["report"]
            metrics["deployment_health"] = report.get("overall_status", "Unknown")
            if "category_results" in report:
                categories = report["category_results"]
                metrics["api_endpoints_working"] = "Working" if categories.get("API Endpoints") else "Issues"
                metrics["database_connected"] = "Connected" if categories.get("System Health") else "Issues"
        
        # Extract from Frontend test
        frontend_test = self.test_results.get("Frontend Automation Test", {})
        if frontend_test.get("report"):
            report = frontend_test["report"]
            metrics["frontend_functionality"] = report.get("overall_status", "Unknown")
            if "category_results" in report:
                categories = report["category_results"]
                metrics["zerodha_auth_status"] = "Working" if categories.get("Zerodha Auth Interface") else "Issues"
        
        # Extract from Zerodha audit
        zerodha_test = self.test_results.get("Zerodha Authentication Audit", {})
        if zerodha_test.get("report"):
            # This would be from the audit report if it exists
            pass
        
        return metrics
    
    def identify_critical_issues(self) -> List[str]:
        """Identify critical issues that need immediate attention"""
        critical_issues = []
        
        # Check each test suite for critical failures
        for test_name, result in self.test_results.items():
            if not result.get("success", False):
                if "error" in result:
                    critical_issues.append(f"{test_name}: {result['error']}")
                else:
                    critical_issues.append(f"{test_name}: Test suite failed")
        
        # Check for specific critical issues from reports
        deployment_test = self.test_results.get("DigitalOcean Deployment Test", {})
        if deployment_test.get("report"):
            report = deployment_test["report"]
            if report.get("score", "0%").rstrip('%') == "0":
                critical_issues.append("Deployment appears to be completely down")
        
        frontend_test = self.test_results.get("Frontend Automation Test", {})
        if frontend_test.get("report"):
            report = frontend_test["report"]
            categories = report.get("category_results", {})
            if not categories.get("Zerodha Auth Interface", True):
                critical_issues.append("Zerodha authentication system is not working properly")
        
        return critical_issues
    
    def generate_recommendations(self) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        # Based on test results, generate specific recommendations
        deployment_test = self.test_results.get("DigitalOcean Deployment Test", {})
        if deployment_test.get("success"):
            if deployment_test.get("report"):
                score = float(deployment_test["report"].get("score", "0%").rstrip('%'))
                if score < 100:
                    recommendations.append("Some API endpoints are returning 404 - check route configuration")
        else:
            recommendations.append("Deployment test failed - check if the app is properly deployed and running")
        
        frontend_test = self.test_results.get("Frontend Automation Test", {})
        if frontend_test.get("success"):
            if frontend_test.get("report"):
                categories = frontend_test["report"].get("category_results", {})
                if not categories.get("Zerodha Auth Interface", True):
                    recommendations.append("Fix Zerodha authentication pages - they are not displaying auth content properly")
                if not categories.get("CORS and Headers", True):
                    recommendations.append("Configure CORS headers properly for frontend-API communication")
        
        zerodha_test = self.test_results.get("Zerodha Authentication Audit", {})
        if not zerodha_test.get("success", True):
            recommendations.append("Set up environment variables for Zerodha API credentials")
            recommendations.append("Ensure Redis server is running for token storage")
            recommendations.append("Install missing dependencies (kiteconnect library)")
        
        # General recommendations
        if not recommendations:
            recommendations.append("System appears healthy - consider monitoring and performance optimization")
        
        return recommendations
    
    def generate_next_steps(self, overall_score: float) -> List[str]:
        """Generate next steps based on overall score"""
        if overall_score >= 90:
            return [
                "‚úÖ System is in excellent health",
                "üîç Monitor system performance regularly",
                "üìà Consider implementing additional monitoring and alerting",
                "üîÑ Plan regular security updates and maintenance"
            ]
        elif overall_score >= 75:
            return [
                "üü° System is generally healthy with minor issues",
                "üîß Address the failed test categories",
                "üìä Monitor the identified issues closely",
                "üõ†Ô∏è Plan fixes for non-critical issues"
            ]
        elif overall_score >= 50:
            return [
                "üü† System has significant issues that need attention",
                "üö® Address critical issues immediately",
                "üîß Fix failing components one by one",
                "üìã Create a remediation plan with priorities"
            ]
        else:
            return [
                "üî¥ System has critical issues requiring immediate attention",
                "üö® Stop production traffic until issues are resolved",
                "üõ†Ô∏è Fix deployment and connectivity issues first",
                "üìû Consider getting external help if needed"
            ]
    
    def print_executive_summary(self, report: Dict):
        """Print executive summary"""
        logger.info("\n" + "üéØ EXECUTIVE SUMMARY")
        logger.info("=" * 50)
        
        overall = report["overall_assessment"]
        logger.info(f"üìä Overall Status: {overall['status']}")
        logger.info(f"üìà Health Score: {overall['score']}")
        logger.info(f"‚úÖ Successful Tests: {overall['successful_tests']}/{overall['total_tests']}")
        
        # Key metrics
        logger.info(f"\nüìã Key Metrics:")
        for metric, value in report["key_metrics"].items():
            logger.info(f"   ‚Ä¢ {metric.replace('_', ' ').title()}: {value}")
        
        # Critical issues
        if report["critical_issues"]:
            logger.info(f"\nüö® Critical Issues:")
            for issue in report["critical_issues"]:
                logger.info(f"   ‚Ä¢ {issue}")
        
        # Top recommendations
        logger.info(f"\nüí° Top Recommendations:")
        for i, rec in enumerate(report["recommendations"][:3], 1):
            logger.info(f"   {i}. {rec}")
        
        # Next steps
        logger.info(f"\nüéØ Next Steps:")
        for step in report["next_steps"]:
            logger.info(f"   {step}")

def main():
    """Main function"""
    base_url = sys.argv[1] if len(sys.argv) > 1 else "https://algoauto-9gx56.ondigitalocean.app"
    
    print(f"üöÄ Complete Deployment Audit")
    print(f"üìç Target: {base_url}")
    print(f"‚è∞ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    auditor = DeploymentAuditor(base_url)
    comprehensive_report = auditor.run_all_tests()
    
    # Save comprehensive report
    report_filename = f"comprehensive_audit_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_filename, 'w') as f:
        json.dump(comprehensive_report, f, indent=2)
    
    logger.info(f"\nüìÑ Comprehensive audit report saved: {report_filename}")
    
    # Exit code based on overall score
    overall_score = float(comprehensive_report["overall_assessment"]["score"].rstrip('%'))
    exit_code = 0 if overall_score >= 70 else 1
    
    if exit_code == 0:
        logger.info("‚úÖ Audit completed successfully")
    else:
        logger.warning("‚ö†Ô∏è Audit completed with issues requiring attention")
    
    return exit_code

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("\n‚ùå Audit interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Audit failed: {e}")
        sys.exit(1) 